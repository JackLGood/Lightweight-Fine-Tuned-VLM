# -*- coding: utf-8 -*-
"""LightweightVLMFinetuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sPlTUdG4urgHu9Bs9pusdmLvJMEleJtl
"""

!pip install --upgrade pip
!pip install transformers datasets accelerate peft bitsandbytes evaluate safetensors pillow

!pip install --upgrade datasets

from datasets import load_dataset

train_ds = load_dataset(
    "ahmed-masry/ChartQA",   # ← HF Hub ID, _not_ a path
    split="train[:10%]",
)
val_ds = load_dataset("ahmed-masry/ChartQA", split="val[:10%]")
test_ds = load_dataset("ahmed-masry/ChartQA", split="test[:10%]")

print(len(train_ds), len(val_ds), len(test_ds))

from itertools import groupby
from datasets import Dataset

# 1) Gather Q&A strings per chart
grouped = {}
for ex in train_ds:
    qa = f"{ex['query']} {ex['label'][0]}"
    grouped.setdefault(ex["imgname"], []).append(qa)

# 2) Build a list of dicts with image + combined caption
caption_examples = []
for imgname, qas in grouped.items():
    # find a matching example to get the PIL image
    img = next(e["image"] for e in train_ds if e["imgname"] == imgname)
    caption_examples.append({
        "imgname": imgname,
        "image": img,
        "caption": " ".join(qas)
    })

# 3) Wrap into a new HF Dataset
caption_ds = Dataset.from_list(caption_examples)
print(caption_ds.column_names, len(caption_ds))

from transformers import BlipProcessor, BlipForConditionalGeneration

MODEL_NAME = "Salesforce/blip-image-captioning-base"
processor = BlipProcessor.from_pretrained(MODEL_NAME)
model     = BlipForConditionalGeneration.from_pretrained(MODEL_NAME)
model = model.to("cuda")

from PIL import Image
import io

def preprocess_fn(ex):
    # Convert image bytes to a PIL Image
    img = Image.open(io.BytesIO(ex["image"]))

    inputs = processor(
        images=img,
        text=ex["caption"],
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    # Trainer expects labels
    inputs["labels"] = inputs.input_ids.clone()
    # squeeze out the batch dim so map works
    return {k: v.squeeze() for k,v in inputs.items()}

# Map over the dataset
train_tok = caption_ds.map(
    preprocess_fn,
    batched=False,
    remove_columns=caption_ds.column_names
)
print(train_tok)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./chart_caption_model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,

    # old names:
    do_eval=True,
    eval_steps=500,

    logging_steps=50,

    save_steps=500,

    learning_rate=5e-5,
    fp16=True,
)

model.save_pretrained("./chart_caption_model")
processor.save_pretrained("./chart_caption_model")

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# 1) Load processor & model
processor = BlipProcessor.from_pretrained("./chart_caption_model")
model     = BlipForConditionalGeneration.from_pretrained("./chart_caption_model")
device    = "cuda" if torch.cuda.is_available() else "cpu"
model     = model.to(device)

# 2) Inference helper with a text prompt
def make_caption(img_path, prompt: str = "", max_new_tokens: int = 100) -> str:
    # — vision side —
    img = Image.open(img_path).convert("RGB")
    vision_inputs = processor(images=img, return_tensors="pt").to(device)

    # — text side —
    prompt_tokens = processor.tokenizer(
        prompt,
        add_special_tokens=True,
        return_tensors="pt"
    ).to(device).input_ids

    # generate, but pass prompt tokens under `input_ids` (not decoder_input_ids)
    output_ids = model.generate(
        pixel_values=vision_inputs.pixel_values,
        input_ids=prompt_tokens,        # <— rename decoder_input_ids → input_ids
        max_new_tokens=max_new_tokens,
        num_beams=5,
        early_stopping=True
    )

    return processor.decode(output_ids[0], skip_special_tokens=True)

# 3) Examples of calling it

# a) Zero-prompt (just what it was trained to do):
print(make_caption("/content/charts/line-chart.png"))